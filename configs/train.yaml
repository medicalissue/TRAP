# Training configuration
name: default

# Training parameters
epochs: 50
batch_size: 64
accumulation_steps: 1  # Gradient accumulation

# Optimizer
optimizer:
  name: adamw
  lr: 1e-4
  weight_decay: 1e-4
  betas: [0.9, 0.999]
  eps: 1e-8

# Learning rate scheduler
scheduler:
  name: cosine  # cosine | step | exponential | none
  warmup_epochs: 5
  min_lr: 1e-6

  # For step scheduler
  step_size: 10
  gamma: 0.1

# Loss weights
losses:
  lambda_mae: 1.0
  lambda_cl: 0.3
  lambda_forecast: 0.1  # Only used if model.heads.use_forecasting=true

# Training data
train_on_normal_only: true
use_augmentation: true

# Data augmentation
augmentation:
  horizontal_flip: 0.5
  temporal_shift: 0.1  # Randomly shift sequence start
  feature_dropout: 0.1  # Dropout on input features
  gaussian_noise: 0.01  # Add gaussian noise to features

# Checkpointing
checkpoint:
  save_every: 5  # Save checkpoint every N epochs
  save_best: true  # Save best model based on validation metric
  monitor: val/total_loss
  mode: min  # min | max
  save_last: true

# Early stopping
early_stopping:
  enabled: false
  patience: 10
  monitor: val/total_loss
  mode: min
  min_delta: 1e-4

# Validation
validation:
  enabled: true
  interval: 1  # Validate every N epochs
  split: 0.1  # Use 10% of training data for validation

# Logging
logging:
  log_every_n_steps: 10
  log_gradients: false
  log_parameters: false
  log_samples: true
  num_samples: 8  # Number of samples to log

# Mixed precision training
mixed_precision:
  enabled: false
  dtype: float16  # float16 | bfloat16

# Distributed training
distributed:
  enabled: false
  backend: nccl
  find_unused_parameters: false
